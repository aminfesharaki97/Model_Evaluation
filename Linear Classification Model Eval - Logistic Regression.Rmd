---
title: "Linear Classification Models - Logistic Regression"
author: "Amin Fesharaki"
date: "6/9/2022"
output: html_document
---
# Question 1

#### The hepatic injury data set was described in the introductory chapter and contains 281 unique compounds, each of which has been classified as causing no liver damage, mild damage, or severe damage. These compounds were analyzed with 184 biological screens (i.e., experiments) to assess each compoundâ€™s effect on a particular biologically relevant target in the body. The larger the value of each of these predictors, the higher the activity of the compound. In addition to biological screens, 192 chemical fingerprint predictors were determined for these compounds. Each of these predictors represent a substructure (i.e., an atom or combination of atoms within the compound) and are either counts of the number of substructures or an indicator of presence or absence of the particular substructure. The objective of this data set is to build a predictive model for hepatic injury so that other compounds can be screened for the likelihood of causing hepatic injury. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(caret)
library(pROC)
library(AppliedPredictiveModeling)
data(hepatic)
```


```{r PreProcessing}
# Pre Process 

# Lump all compounds that cause injury into a "Yes" category:
any_damage = as.character( injury )
any_damage[ any_damage=="Mild" ] = "Yes"
any_damage[ any_damage=="Severe" ] = "Yes"
any_damage[ any_damage=="None" ] = "No"

# Convert our response to a factor (make the first factor correspond to the event of interest):
any_damage = factor( any_damage, levels=c("Yes","No")) 

#The data frames bio and chem contain the biological assay and chemical fingerprint predictors for the 281 compounds, while the factor variable injury contains the liver damage classification for each compound.
```

## a)	Given the classification imbalance in hepatic injury status, describe how you would create a training and testing set. 
```{r}
table(injury)
table(any_damage)
```
There are 3 categories for injury: none(106), mild (145), severe (30). In order to reduce class imbalance and reduce the number of responses to create a binary classification model, mild and severe will be set to "yes" and none will be set to "no". Now the 'yes' class has 175 obs and 'no' has 106. To help control the class imbalance, we will be using *stratified sampling* to create a more balanced class distribution.     

## b)	Which classification statistic would you choose to optimize for this exercise and why? 
There are 3 different metrics to consider when deciding the best model: accuracy, sensitivity, and specificity. In regards to health models, it is often more important to choose a model with a higher sensitivity or specificity rather than overall accuracy. In summary, sensitivity tests its ability to correctly identify those with liver damage, while specificity tests its ability to correctly identify those without the liver damage. In the case of the hepatic data, it is more important to accurately predict 'yes' values than 'no' values. Predicting 'yes' corresponds to someone having hepatic injury which is a serious health concern. Therefore, a false negative should be considered more than a false positive. Since we want to minimize false negative results, *sensitivity* should be optimized.

**Overview**  
*Important to understand for model results (Confusion Matrix)*  
*Injury = 'yes' = 'Positive' 
*No Injury = 'no' = 'Negative'
*True Positive - Predicted compound IS hepatic || Real compound actually IS hepatic
*True Negative - Predicted compound is NOT hepatic || Real compound is actually NOT hepatic
*False Positive - Predicted compound IS hepatic || Real  compound is actually NOT hepatic
*False Negative - Predicted compound in actually NOT hepatic || Real compound is actually hepatic  

**Reasoning**   
*Sensitivity* measures the proportion of positive test results of all true samples. Therefore, test's sensitivity correctly identify observations true positive (liver damage), while minimizing the number of false negatives. In a similar real world scenario, it is less harmful/impact to have a false positive than a false negative since the compound isn't really in danger. If a false positive were to happen and the compound looks fine, then a doctor would normally retest the compound to double check. However, a false negative puts the compound in real danger by diagnosing the compound is healthy, but in reality, the compound has hepatic injury.    

## c)	Split the data into a training and a testing set, pre-process the data, and build models described in this chapter for the biological predictors and separately for the chemical fingerprint predictors. Which model has the best predictive ability for the biological predictors, and what is the optimal performance? Which model has the best predictive ability for the chemical predictors, and what is the optimal performance? Based on these results, which set of predictors contains the most information about hepatic toxicity? 

### Split
```{r Split}
#Split Data Set into train & test
set.seed(1)

#Stratified Sample Partition with a  80/20 split
Sample <- createDataPartition(any_damage, p=.8, list=FALSE)

#Split Data into Test and Train Data Sets - Biological (Predictors)
bio_train_x <- bio[Sample,]
bio_test_x <- bio[-Sample,]

#Split Data into Test and Train Data Sets - Chemical (Predictors)
chem_train_x <- chem[Sample,]
chem_test_x <- chem[-Sample,]


#Split Data into Test and Train Data Sets - Damage (Response)
dmg_train_y <- any_damage[Sample]
dmg_test_y <- any_damage[-Sample]
```

### Pre-Process
```{r PreProcess}
set.seed(1)
#PreProcess the data (Knn Impute, Near Zero Variance, and Linear Combos)

#Biological
tran_bio <- preProcess(bio_train_x,
                    method = c( "knnImpute", 
                    "nzv"))
bio_train_xTran <- predict(tran_bio,bio_train_x)
bio_test_xTran <- predict(tran_bio,bio_test_x)
#Chemical
tran_chem <- preProcess(chem_train_x,
                    method = c(  "knnImpute", 
                    "nzv"))
chem_train_xTran <- predict(tran_chem,chem_train_x)
chem_test_xTran <- predict(tran_chem,chem_test_x)

# Remove Collinearity 
bio_collinear <- findLinearCombos(bio) # No linear Combos for biological data
chem_collinear <- findLinearCombos(chem)$remove
chem_train_xTran <- chem_train_xTran[-c(chem_collinear)]
chem_test_xTran <- chem_test_xTran[-c(chem_collinear)]
```

### Build Models

```{r Control}
#Set Control - Cross Validation
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)
```

#### Generalized Linear Model
```{r GLM, warning = FALSE}
set.seed(1)
#Create glm model

bio_glmFit <- train(x = bio_train_xTran, 
                y = dmg_train_y,
                method = "glm",
                preProc = c("center","scale"),
                metric = "Sens",
                trControl = ctrl)

chem_glmFit <- train(x = chem_train_xTran, 
                y = dmg_train_y,
                method = "glm",
                preProc = c("center","scale"),
                metric = "Sens",
                trControl = ctrl)

#Store Predictions
bio_testResults <- data.frame(obs = dmg_test_y)
bio_testResults$GLM <- predict(bio_glmFit, bio_test_xTran)

chem_testResults <- data.frame(obs = dmg_test_y)
chem_testResults$GLM <- predict(chem_glmFit, chem_test_xTran)
```

#### Linear Discriminant Analysis
```{r LDA, warning = FALSE}
set.seed(1)
#Create LDA model

bio_ldaFit <- train(x = bio_train_xTran, 
                y = dmg_train_y,
                method = "lda",
                preProc = c("center","scale"),
                metric = "Sens",
                trControl = ctrl)

chem_ldaFit <- train(x = chem_train_xTran, 
                y = dmg_train_y,
                method = "lda",
                preProc = c("center","scale"),
                metric = "Sens",
                trControl = ctrl)

#Store Predictions
bio_testResults$LDA <- predict(bio_ldaFit, bio_test_xTran)
chem_testResults$LDA <- predict(chem_ldaFit, chem_test_xTran)
```

#### Penalized Logistic Regression
```{r Pen, warning = FALSE}
glmnGrid <- expand.grid(alpha = c(0,  .1,  .2, .4, .5, .6, .8, .9, 1),
                        lambda = seq(.01, .3, length = 20))

set.seed(1)
#Create glmnet model

bio_glmnFit <- train(x = bio_train_xTran, 
                y = dmg_train_y,
                method = "glmnet",
                tuneGrid = glmnGrid,
                preProc = c("center","scale"),
                metric = "ROC", #ROC is used instead of Sens
                trControl = ctrl) 

# metric = Sens results in a model with Yes predictions for all  (Sens=1 / Spec = 0), 
# therefore, ROC will be used instead, but FN will be looked at

chem_glmnFit <- train(x = chem_train_xTran, 
                y = dmg_train_y,
                method = "glmnet",
                preProc = c("center","scale"),
                metric = "ROC", #ROC is used instead of Sens
                trControl = ctrl)

#Store Predictions
bio_testResults$GLMnet <- predict(bio_glmnFit, bio_test_xTran)
chem_testResults$GLMnet <- predict(chem_glmnFit, chem_test_xTran)
```

#### Nearest Shrunken Centroids 
```{r NSC}
set.seed(1)
#Create Nearest Shrunken Centroid model

bio_nscFit <- train(x = bio_train_xTran, 
                y = dmg_train_y,
                method = "pam",
                preProc = c("center","scale"),
                metric = "ROC", #ROC is used instead of Sens
                tuneGrid = data.frame(threshold = seq(0, 2, length = 50)),
                trControl = ctrl)

# metric = Sens results in a model with Yes predictions for all  (Sens=1 / Spec = 0), 
# therefore, ROC will be used instead, but FN will be looked at

chem_nscFit <- train(x = chem_train_xTran, 
                y = dmg_train_y,
                method = "pam",
                preProc = c("center","scale"),
                metric = "ROC", #ROC is used instead of Sens
                tuneGrid = data.frame(threshold = seq(0, 2, length = 50)),
                trControl = ctrl)

#Store Predictions
bio_testResults$NSC <- predict(bio_nscFit, bio_test_xTran)
chem_testResults$NSC <- predict(chem_nscFit, chem_test_xTran)

```

### Model Performance Comparison

```{r comparemodels Bio}
set.seed(1)
### Compare Models using confusion matrix - Biological

confusionMatrix(bio_testResults$GLM, dmg_test_y, positive = "Yes")
confusionMatrix(bio_testResults$LDA, dmg_test_y, positive = "Yes")
confusionMatrix(bio_testResults$GLMnet, dmg_test_y, positive = "Yes")
confusionMatrix(bio_testResults$NSC, dmg_test_y, positive = "Yes")
```


```{r comparemodels Chem}
set.seed(1)
### Compare Models using confusion matrix - Chemical

confusionMatrix(chem_testResults$GLM, dmg_test_y, positive = "Yes")
confusionMatrix(chem_testResults$LDA, dmg_test_y, positive = "Yes")
confusionMatrix(chem_testResults$GLMnet, dmg_test_y, positive = "Yes")
confusionMatrix(chem_testResults$NSC, dmg_test_y, positive = "Yes")
```
###***DELETE ME - Explain better but best model is NSC due to the least amount of false negatives cuz false negatives is badddd in health - EXPLAIN SIMILAR SHIT FOR ALL SIMILAR SECTIONS

###Which model has the best predictive ability for the biological predictors, and what is the optimal performance?
```{r Best Bio}
# Best Model for Biological - Nearest Shrunken Centroids 
confusionMatrix(bio_testResults$NSC, dmg_test_y, positive = "Yes")
# Final Model 
bio_nscFit$finalModel
```
The Nearest Shrunken Centroid with an optimal threshold value of 1.224  
###Which model has the best predictive ability for the chemical predictors, and what is the optimal performance?

```{r Best Chem}
# Best Model for Chemical - Nearest Shrunken Centroids 
confusionMatrix(chem_testResults$NSC, dmg_test_y, positive = "Yes")
# Final Model
chem_nscFit$finalModel
```

## d)	For the optimal models for both the biological and chemical predictors, what are the top five important predictors? 
```{r VarImpBio}
set.seed(1)
#Variable Importance for Biological NSC (Optimal Model)
bio_imp <- varImp(bio_nscFit)
plot(bio_imp, top = 5, main = "Top 5 Important Biological Predictors")
```

```{r VarImpChem}
set.seed(1)
#Variable Importance for Chemical NSC (Optimal Model)
chem_imp <- varImp(chem_nscFit)
plot(chem_imp, top = 5, main = "Top 5 Important Chemical Predictors")
```

## e.1)	Now combine the biological and chemical fingerprint predictors into one predictor set. Retrain the same set of predictive models you built from part (c). Which model yields the best predictive performance? Is the model performance better than either of the best models from part (c)? 
```{r Merge}
set.seed(1)
#Combine Biological and Chemical Data
both_train <- cbind(bio_train_xTran, chem_train_xTran)
both_test <- cbind(bio_test_xTran, chem_test_xTran)
```

#### Generalized Linear Model
```{r GLM, warning = FALSE}
set.seed(1)
#Create glm model

both_glmFit <- train(x = both_train, 
                y = dmg_train_y,
                method = "glm",
                preProc = c("center","scale"),
                metric = "Sens",
                trControl = ctrl)

#Store Predictions
both_testResults <- data.frame(obs = dmg_test_y)
both_testResults$GLM <- predict(both_glmFit, both_test)


```

#### Linear Discriminant Analysis
```{r LDA, warning = FALSE}
set.seed(1)
#Create LDA model

both_ldaFit <- train(x = both_train, 
                y = dmg_train_y,
                method = "lda",
                preProc = c("center","scale"),
                metric = "Sens",
                trControl = ctrl)

#Store Predictions
both_testResults$LDA <- predict(both_ldaFit, both_test)
```

#### Penalized Logistic Regression
```{r Pen, warning = FALSE}
glmnGrid <- expand.grid(alpha = c(0,  .1,  .2, .4, .5, .6, .8, .9, 1),
                        lambda = seq(.01, .3, length = 30))

set.seed(1)
#Create glmnet model

both_glmnFit <- train(x = both_train, 
                y = dmg_train_y,
                method = "glmnet",
                tuneGrid = glmnGrid,
                preProc = c("center","scale"),
                metric = "ROC", #ROC is used instead of Sens
                trControl = ctrl)

# metric = Sens results in a model with Yes predictions for nearly all  (Sens=~1 / Spec = ~0), 
# therefore, ROC will be used instead, but FN will be looked at

#Store Predictions
both_testResults$GLMnet <- predict(both_glmnFit, both_test)
```

#### Nearest Shrunken Centroids 
```{r}
set.seed(1)
#Create Nearest Shrunken Centroid model

both_nscFit <- train(x = both_train, 
                y = dmg_train_y,
                method = "pam",
                preProc = c("center","scale"),
                metric = "ROC", #ROC is used instead of Sens
                tuneGrid = data.frame(threshold = seq(0, 2, length = 30)),
                trControl = ctrl)
# metric = Sens results in a model with Yes predictions for all  (Sens=1 / Spec = 0), 
# therefore, ROC will be used instead, but FN will be looked at

#Store Predictions
both_testResults$NSC <- predict(both_nscFit, both_test)

```

### Model Performance Comparison

```{r comparemodels both}
set.seed(1)
### Compare Models using confusion matrix - Both (Biological and Chemical)
confusionMatrix(both_testResults$GLM, dmg_test_y, positive = "Yes")
confusionMatrix(both_testResults$LDA, dmg_test_y, positive = "Yes")
confusionMatrix(both_testResults$GLMnet, dmg_test_y, positive = "Yes")
confusionMatrix(both_testResults$NSC, dmg_test_y, positive = "Yes")
```


```{r Best Both}
set.seed(1)
# Best Model for Both data sets combined - NSC 
confusionMatrix(both_testResults$NSC, dmg_test_y, positive = "Yes")
# Final Model
both_nscFit$finalModel
```
## e.2) What are the top five important predictors for the optimal model? How do these compare with the optimal predictors from each individual predictor set?

```{r VarImpBio}
set.seed(1)
#Variable Importance for NSC (Optimal Model)
both_imp <- varImp(both_nscFit)
plot(both_imp, top = 5, main = "Top 5 Important Predictors")
```

## f)	Which model (either model of individual biology or chemical fingerprints or the combined predictor model), if any, would you recommend using to predict compoundsâ€™ hepatic toxicity? Explain. 


## ***ASK OR CONSIDER THIS*** -- 2 models, 1 predicts yes for everything but never predicits a false negative, or something a little bit more accurate but does predict false negative.
Another consideration - Is it better to tune for ROC and look at smallest FN or tune for Sens and minimize FN


#Variable Importance
```{r, fig.width=2, fig.height=2, warning = FALSE}

#Variable Importance for each Model - Bio

bio_imp_glm <- varImp(bio_glmFit)
bio_imp_lda <- varImp(bio_ldaFit)
bio_imp_glmn <- varImp(bio_glmnFit)
bio_imp_nscFit <- varImp(bio_nscFit)

# divide window into a 2X2 grid
par(mfrow= c(2,2) )
 
# add plots to window
plot( bio_imp_glm, main = "Top 5 Imp. Bio Predictors-LR", top = 5)
plot( bio_imp_lda, main = "Top 5 Imp. Biol Predictors-LDA", top = 5 )
plot( bio_imp_glmn, main = "Top 5 Imp. Bio Predictors-GLMnet", top = 5 )
plot( bio_imp_nscFit, main = "Top 5 Imp. Bio Predictors -NSC", top = 5 )

#Variable Importance for each Model - Chem

chem_imp_glm <- varImp(chem_glmFit)
chem_imp_lda <- varImp(chem_ldaFit)
chem_imp_glmn <- varImp(chem_glmnFit)
chem_imp_nscFit <- varImp(chem_nscFit)

# divide window into a 2X2 grid
par(mfrow= c(2,2) )

# add plots to window
plot( chem_imp_glm, main = "Top 5 Imp. Chem Predictors-LR", top = 5 )
plot( chem_imp_lda, main = "Top 5 Imp. Chem Predictors-LDA", top = 5 )
plot( chem_imp_glmn, main = "Top 5 Imp. Chem Predictors-GLMnet", top = 5 )
plot( chem_imp_nscFit, main = "Top 5 Imp. Chem Predictors-NSC", top = 5 )
```
